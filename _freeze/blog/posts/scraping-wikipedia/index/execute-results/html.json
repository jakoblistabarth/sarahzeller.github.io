{
  "hash": "03fe005e536570be45854cad6baf9ec1",
  "result": {
    "engine": "knitr",
    "markdown": "---\nauthor: Sarah Zeller\ntitle: Scraping wikipedia\ndate: \"2025-05-28\"\ncategories: [code, summary]\ncitation:\n  url: https://sarahzeller.github.io/blog/posts/scraping-wikipedia/\n\nformat:\n  html:\n    toc: true\n\nexecute:\n  warning: false\n---\n\n\n\nEvery now and then, we want to get some data from the wild -- e.g., from Wikipedia articles.\nWe'll see two data types here:\n\n- tables\n- specific data from a specific class.\n\nBefore we start, let's load the necessary libraries.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for scraping\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(parzer)\n\n# to parse it to spatial data\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)\n```\n:::\n\n\n\n# Scraping multiple tables\n\nLet's start with a simple table to scrape.\nHere, you see the URL that we're working with.\nIf you check it out, you can see that we've got multiple tables in here.\nOur aim is to collect them all into a single table.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://en.wikipedia.org/wiki/List_of_NATO_installations_in_Afghanistan\"\n```\n:::\n\n\n\n\nSo, let's start by reading in the table.\nWe'll discard everything that isn't a table (i.e. just one column).\nIn this specific format, we'll also get the name of each table, since that seems to contain information.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntables <- url %>%   \n  read_html() |>\n  html_table() |> \n  keep(~ncol(.x) > 1) \n\ntable_names <- tables |> \n  map(~names(.x) |> unique()) |> \n  unlist() |> \n  discard(~.x == \"\")\n```\n:::\n\n\n\nThe problem with our data so far is that the actual variable names are in the first row of our `data.frame`s.\nSo let's write a function to get the variable names from the first row.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_first_row_to_col_names <- function(data){\n  col_names <- data[1,] |> as.character()\n  data[-1, ] |> \n    setNames(col_names)\n}\n```\n:::\n\n\n\nNow that we've written this function, we can bring everything together:\n\n- the tables\n- with their correct variable names and\n- their table origin\n\nWe'll use the `map2` function here, since we're basically looping over two lists:\n\n- tables\n- table origin\n\nThe `~` indicates that we're creating a function on the fly here, where we're taking each table, correcting their variable names and adding the table origin.\nWe'll them bring them all into one table.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntables_merged <- map2(tables,\n                      table_names,\n                      ~ .x |> make_first_row_to_col_names() |> mutate(origin = .y)) |> \n  plyr::rbind.fill()\n\ntables_merged |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Type                                  Name       District Opened   Closed\n1 Camp HKIA/KAIA(Hamid Karzai Int'l Airport) Kabul District   2005         \n2 Camp                                 Alamo Kabul District                \n3 Camp                           Bala Hissar Kabul District                \n4 Camp                           Black Horse Kabul District   2008         \n5 Camp                                 Dogan Kabul District   2002 Feb 2015\n6 Camp                                  Dubs Kabul District                \n                                                                                                                                         Forces\n1 ISAF Headquarters ISAF Joint Command Turkish Army US Army USMC US Air Force Australian Army British Army Canadian Army Mongolian Armed Forces\n2                                                                                                                       US Army Australian Army\n3                                                                                                                                       US Army\n4                                                                                                                         US Army Canadian Army\n5                                                                                                                                  Turkish Army\n6                                                                                                                                              \n                                                                                         Notes\n1 ISAF HeadquartersISAF Joint Command HeadquartersHeadquarters for RC-Capital[citation needed]\n2                                             NATO Training Mission. Soldiers and Contractors.\n3                                                                                          [6]\n4                                                                                          [7]\n5                                                                                          [8]\n6                                                                                          [9]\n                        origin NA\n1 Kabul Province Installations NA\n2 Kabul Province Installations NA\n3 Kabul Province Installations NA\n4 Kabul Province Installations NA\n5 Kabul Province Installations NA\n6 Kabul Province Installations NA\n```\n\n\n:::\n:::\n\n\n\n\n# Scraping coordinates\n\nSometimes, we want to get very specific data from a web page.\nIn this case, we want to get coordinates from Wikipedia district websites.\nLet's check out the two districts and their Wikipedia sites.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistrict_urls <- tibble::tribble(~ district, ~ url,\n                                 \"Guzargahi-Nur\", \"https://en.wikipedia.org/wiki/Guzargahi_Nur_District\",\n                                 \"Puli-Khumri\", \"https://en.wikipedia.org/wiki/Puli_Khumri\")\ndistrict_urls\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  district      url                                                 \n  <chr>         <chr>                                               \n1 Guzargahi-Nur https://en.wikipedia.org/wiki/Guzargahi_Nur_District\n2 Puli-Khumri   https://en.wikipedia.org/wiki/Puli_Khumri           \n```\n\n\n:::\n:::\n\n\n\nBy checking these websites out, we've already found out that we're searching for the `class` `geo-dms`.\nThis is something we can search for with `html_element`.\nLet's make a function that will scrape the coordinates from these URLs using this class.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_coordinates_from_url <- function(wikipedia_url) {\n  wikipedia_url |>\n    read_html() |>\n    html_element(\".geo-dms\") |>\n    html_text() \n}\n```\n:::\n\n\n\nThen, we can use this function on our `district_urls` `data.set` to create a new `character` variable: `coordinates`.\nImportantly, we have to do this once per row, so we need to use the function `rowwise` and later on `ungroup`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistricts <- district_urls |> \n  rowwise() |> \n  mutate(coordinates = get_coordinates_from_url(url)) |> \n  ungroup()\n\ndistricts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  district      url                                                  coordinates\n  <chr>         <chr>                                                <chr>      \n1 Guzargahi-Nur https://en.wikipedia.org/wiki/Guzargahi_Nur_District 36°13′48″N…\n2 Puli-Khumri   https://en.wikipedia.org/wiki/Puli_Khumri            35°57′N 68…\n```\n\n\n:::\n:::\n\n\n\nNow we've got the coordinates, but they're not yet in a format we can use.\nTo change this, we'll use the `parzer` library.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistricts_with_coordinates <- districts$coordinates |> \n  parzer::parse_llstr() |> \n  cbind(districts) |> \n  select(district, lat, lon)\n\ndistricts_with_coordinates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       district   lat   lon\n1 Guzargahi-Nur 36.23 69.57\n2   Puli-Khumri 35.95 68.70\n```\n\n\n:::\n:::\n\n\n\n## Parsing to sf\n\nLastly, we'll want to parse this to an `sf` geometry.\nWe can easily do this with its `st_as_sf` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistricts_sf <- districts_with_coordinates |> \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n```\n:::\n\n\n\n## Mapping as a sanity check\n\nJust to check if our two points are indeed within Afghanistan, we'll draw a map with them.\nFor this, we first need to load the Afghanistan geometry.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafghanistan <- ne_countries(country = \"Afghanistan\", scale = \"small\", returnclass = \"sf\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_sf(data = afghanistan) + \n  geom_sf(data = districts_sf) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-afghanistan-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that the two points are within Afghanistan, so we seem to have mapped it correctly. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}